<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tongxu Luo | Homepage</title>
  <meta name="author" content="Tongxu Luo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜†</text></svg>">
</head>

<body>
  <table style="width:100%; max-width:800px; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%; width:63%; vertical-align:middle">
                  <p style="text-align:center">
                    <name>Tongxu Luo</name>
                  </p>
                  <p>
                    I am Tongxu Luo, a third-year undergraduate student at USTB. B.S. Candidate in Computer Science and Technology.
                    <!-- <a href="[Advisor's Website URL]">Advisor's Name</a>. -->
                  </p>
                  <p>
                    Research Intern at CASIA, mentored by Dr. Shizhu He during my sophomore year (Mar 2023 - Nov 2023).
                  </p>
                  <p>
                    Research Intern at HKUST, under the guidance of Dr. Jie Fu (Nov 2023 - ).
                  </p>
                  <p>
                    My current research interests include Large Language Models, Continual Learning, Model Growth and Parameter-Efficient Fine-Tuning. 
                  </p>

                  <p style="text-align:left">
                    <a href="mailto:u202142005@xs.ustb.edu.cn"><img src="imgs/mail.svg" alt="Email" style="vertical-align:middle;" width="24" height="24"></a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=aJEGhzkAAAAJ"><img src="imgs/google-scholar.svg" alt="Google Scholar" style="vertical-align:middle;" width="24" height="24"></a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/kuro_xiro"><img src="imgs/twitter.svg" alt="Twitter" style="vertical-align:middle;" width="30" height="30"></a> &nbsp;/&nbsp;
                    <a href="https://github.com/tongxuluo"><img src="imgs/github-alt.svg" alt="GitHub" style="vertical-align:middle;" width="24" height="24"></a>
                  </p>
                </td>
                <td style="padding:2.5%; width:25%; max-width:25%">
                  <a href="imgs/ltx.jpg"><img style="width:125%; max-width:125%" alt="profile photo" src="imgs/ltx.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px; width:100%; vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>

              <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.15319">
                  <papertitle>Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training</papertitle>
                  </a>
                  <br>
                  Wenyu Du*, <strong>Tongxu Luo*</strong>, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu
                  <br>
                  <strong><em>Preprint</em></strong>, 2024
                  <br>
              </td>
              </tr>

                <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2402.13717">
                    <papertitle>Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent</papertitle>
                    </a>
                    <br>
                    Xiaoyan Yu*, <strong>Tongxu Luo*</strong>, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu
                    <br>
                    <strong><em>Preprint</em></strong>, 2024
                    <br>
                </td>
                </tr>

                <!-- <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2402.12851">
                    <papertitle>MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models</papertitle>
                    </a>
                    <br>
                    <strong>Tongxu Luo*</strong>, Jiahe Lei*, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, Kang Liu
                    <br>
                    <strong><em>Preprint</em></strong>, 2024
                    <br>
                </td>
                </tr> -->

                <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2401.11944">
                    <papertitle>CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark</papertitle>
                    </a>
                    <br>
                    Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, <strong>Tongxu Luo</strong>, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu
                    <br>
                    <strong><em>Preprint</em></strong>, 2024
                    <br>
                </td>
                </tr>
              
                <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612850">
                    <papertitle>Answer-Based Entity Extraction and Alignment for Visual Text Question Answering</papertitle>
                    </a>
                    <br>
                    Jun Yu, Mohan Jing, Weihao Liu*, <strong>Tongxu Luo*</strong>, Bingyuan Zhang, Keda Lu, Fangyu Lei, Jianqing Sun, Jiaen Liang
                    <br>
                    <strong><em>ACMMM</em></strong>, 2023
                    <br>
                </td>
                </tr>

                <!-- <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2309.12669">
                    <papertitle>HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering</papertitle>
                    </a>
                    <br>
                    <strong>Tongxu Luo</strong>, Fangyu Lei, Jiahe Lei, Weihao Liu, Shihu He, Jun Zhao, Kang Liu
                    <br>
                    <strong><em>Preprint</em></strong>, 2023
                    <br>
                </td>
                </tr> -->

                <tr>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2310.15075">
                        <papertitle>TableQAKit: A Comprehensive and Practical Toolkit for Table-based Question Answering</papertitle>
                    </a>
                    <br>
                    Fangyu Lei*, <strong>Tongxu Luo*</strong>, Pengqi Yang*, Weihao Liu*, Shizhu He, Jun Zhao, Kang Liu
                    <br>
                    <strong><em>Preprint</em></strong>, 2023
                    <br>
                    </td>
                </tr>

                <tr>
                <td style="padding:20px; width:75%; vertical-align:middle">
                    <a href="https://arxiv.org/abs/2309.04790">
                    <papertitle>MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images</papertitle>
                    </a>
                    <br>
                    Weihao Liu, Fangyu Lei, <strong>Tongxu Luo</strong>, Jiahe Lei, Shizhu He, Jun Zhao, Kang Liu
                    <br>
                    <strong><em>Preprint</em></strong>, 2023
                    <br>
                </td>
                </tr>
                <!-- Add more publication entries as needed -->

            </tbody>
          </table>
        
          <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px; width:75%; vertical-align:middle">
                  <heading>Scholarships</heading>
                </td>
              </tr>
              <tr>
                <td style="padding:20px; width:75%; vertical-align:middle">
                  Awarded the National Scholarship for the 2022-2023 academic year.
                </td>
              </tr>
              <!-- Add more scholarship entries as needed -->
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
